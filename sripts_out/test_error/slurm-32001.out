/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
04/23/2024 16:55:47 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1997] 2024-04-23 16:55:47,014 >> PyTorch: setting up devices
/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/transformers/training_args.py:1910: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
04/23/2024 16:55:47 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
04/23/2024 16:55:47 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=model_save/Mistral/runs/Apr23_16-55-46_a40c05,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=model_save/Mistral,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=model_save/Mistral,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
sortish_sampler=False,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
04/23/2024 16:55:47 - INFO - llmtuner.dsets.loader - Loading dataset alpaca_gpt4_data_zh.json...
/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/datasets/load.py:2547: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
Using custom data configuration default-43a6a4f5f63bb339
Loading Dataset Infos from /share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/datasets/packaged_modules/json
Generating dataset json (/share/home/22351111/.cache/huggingface/datasets/json/default-43a6a4f5f63bb339/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7)
Downloading and preparing dataset json/default to /share/home/22351111/.cache/huggingface/datasets/json/default-43a6a4f5f63bb339/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 48818 examples [00:00, 139848.81 examples/s]Generating train split: 48818 examples [00:00, 133802.57 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /share/home/22351111/.cache/huggingface/datasets/json/default-43a6a4f5f63bb339/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7. Subsequent calls will reuse this data.
[INFO|tokenization_utils_base.py:2085] 2024-04-23 16:57:07,839 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2085] 2024-04-23 16:57:07,839 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2085] 2024-04-23 16:57:07,839 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2085] 2024-04-23 16:57:07,839 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2085] 2024-04-23 16:57:07,839 >> loading file tokenizer_config.json
[WARNING|logging.py:314] 2024-04-23 16:57:07,922 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:314] 2024-04-23 16:57:08,302 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:724] 2024-04-23 16:57:08,307 >> loading configuration file ../pretrain/Mistral-7B-OpenOrca/config.json
[INFO|configuration_utils.py:789] 2024-04-23 16:57:08,309 >> Model config MistralConfig {
  "_name_or_path": "../pretrain/Mistral-7B-OpenOrca",
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 32000,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 10000.0,
  "sliding_window": 4096,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.40.0",
  "use_cache": true,
  "vocab_size": 32002
}

[INFO|modeling_utils.py:3426] 2024-04-23 16:57:08,364 >> loading weights file ../pretrain/Mistral-7B-OpenOrca/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1494] 2024-04-23 16:57:08,365 >> Instantiating MistralForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-04-23 16:57:08,365 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:10<00:10, 10.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  6.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:14<00:00,  7.21s/it]
[INFO|modeling_utils.py:4170] 2024-04-23 16:57:23,564 >> All model checkpoint weights were used when initializing MistralForCausalLM.

[INFO|modeling_utils.py:4178] 2024-04-23 16:57:23,564 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at ../pretrain/Mistral-7B-OpenOrca.
If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.
[INFO|configuration_utils.py:881] 2024-04-23 16:57:23,567 >> loading configuration file ../pretrain/Mistral-7B-OpenOrca/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-23 16:57:23,567 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 32000
}

04/23/2024 16:57:23 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
04/23/2024 16:57:23 - INFO - llmtuner.tuner.core.loader - trainable params: 3407872 || all params: 7245156352 || trainable%: 0.0470
04/23/2024 16:57:23 - INFO - llmtuner.extras.template - Add pad token: <unk>
Filter:   0%|          | 0/48818 [00:00<?, ? examples/s]Caching processed dataset at /share/home/22351111/.cache/huggingface/datasets/json/default-43a6a4f5f63bb339/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-8af29098388ad2d8.arrow
Filter:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/48818 [00:00<00:00, 187962.32 examples/s]Filter:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40000/48818 [00:00<00:00, 191575.17 examples/s]Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48818/48818 [00:00<00:00, 189534.68 examples/s]
Running tokenizer on dataset:   0%|          | 0/48818 [00:00<?, ? examples/s]Caching processed dataset at /share/home/22351111/.cache/huggingface/datasets/json/default-43a6a4f5f63bb339/0.0.0/c8d2d9508a2a2067ab02cd118834ecef34c3700d143b31835ec4235bf10109f7/cache-9aa23581a8db3a5d.arrow
Running tokenizer on dataset:   2%|â–         | 1000/48818 [00:00<00:19, 2407.67 examples/s]Running tokenizer on dataset:   4%|â–         | 2000/48818 [00:00<00:20, 2318.54 examples/s]Running tokenizer on dataset:   6%|â–Œ         | 3000/48818 [00:01<00:19, 2341.16 examples/s]Running tokenizer on dataset:   8%|â–Š         | 4000/48818 [00:01<00:19, 2318.82 examples/s]Running tokenizer on dataset:  10%|â–ˆ         | 5000/48818 [00:02<00:19, 2302.82 examples/s]Running tokenizer on dataset:  12%|â–ˆâ–        | 6000/48818 [00:02<00:18, 2314.23 examples/s]Running tokenizer on dataset:  14%|â–ˆâ–        | 7000/48818 [00:03<00:18, 2299.76 examples/s]Running tokenizer on dataset:  16%|â–ˆâ–‹        | 8000/48818 [00:03<00:17, 2286.75 examples/s]Running tokenizer on dataset:  18%|â–ˆâ–Š        | 9000/48818 [00:03<00:17, 2285.72 examples/s]Running tokenizer on dataset:  20%|â–ˆâ–ˆ        | 10000/48818 [00:04<00:16, 2299.84 examples/s]Running tokenizer on dataset:  23%|â–ˆâ–ˆâ–       | 11000/48818 [00:04<00:16, 2277.00 examples/s]Running tokenizer on dataset:  25%|â–ˆâ–ˆâ–       | 12000/48818 [00:05<00:16, 2279.64 examples/s]Running tokenizer on dataset:  27%|â–ˆâ–ˆâ–‹       | 13000/48818 [00:05<00:15, 2280.22 examples/s]Running tokenizer on dataset:  29%|â–ˆâ–ˆâ–Š       | 14000/48818 [00:06<00:15, 2276.61 examples/s]Running tokenizer on dataset:  31%|â–ˆâ–ˆâ–ˆ       | 15000/48818 [00:06<00:14, 2281.87 examples/s]Running tokenizer on dataset:  33%|â–ˆâ–ˆâ–ˆâ–      | 16000/48818 [00:06<00:14, 2269.86 examples/s]Running tokenizer on dataset:  35%|â–ˆâ–ˆâ–ˆâ–      | 17000/48818 [00:07<00:14, 2258.25 examples/s]Running tokenizer on dataset:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 18000/48818 [00:07<00:13, 2254.07 examples/s]Running tokenizer on dataset:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 19000/48818 [00:08<00:13, 2236.22 examples/s]Running tokenizer on dataset:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20000/48818 [00:08<00:12, 2252.99 examples/s]Running tokenizer on dataset:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21000/48818 [00:09<00:12, 2262.56 examples/s]Running tokenizer on dataset:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 22000/48818 [00:09<00:11, 2256.09 examples/s]Running tokenizer on dataset:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 23000/48818 [00:10<00:11, 2265.81 examples/s]Running tokenizer on dataset:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 24000/48818 [00:10<00:10, 2268.40 examples/s]Running tokenizer on dataset:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25000/48818 [00:10<00:10, 2248.77 examples/s]Running tokenizer on dataset:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26000/48818 [00:11<00:10, 2254.61 examples/s]Running tokenizer on dataset:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 27000/48818 [00:11<00:09, 2235.67 examples/s]Running tokenizer on dataset:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 28000/48818 [00:12<00:09, 2237.40 examples/s]Running tokenizer on dataset:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 29000/48818 [00:12<00:08, 2236.70 examples/s]Running tokenizer on dataset:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 30000/48818 [00:13<00:08, 2246.06 examples/s]Running tokenizer on dataset:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31000/48818 [00:13<00:07, 2247.14 examples/s]Running tokenizer on dataset:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 32000/48818 [00:14<00:07, 2255.70 examples/s]Running tokenizer on dataset:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 33000/48818 [00:14<00:06, 2261.64 examples/s]Running tokenizer on dataset:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 34000/48818 [00:14<00:06, 2245.67 examples/s]Running tokenizer on dataset:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 35000/48818 [00:15<00:06, 2257.37 examples/s]Running tokenizer on dataset:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36000/48818 [00:15<00:05, 2255.07 examples/s]Running tokenizer on dataset:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 37000/48818 [00:16<00:05, 2267.49 examples/s]Running tokenizer on dataset:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 38000/48818 [00:16<00:04, 2254.04 examples/s]Running tokenizer on dataset:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 39000/48818 [00:17<00:04, 2266.91 examples/s]Running tokenizer on dataset:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 40000/48818 [00:17<00:03, 2261.80 examples/s]Running tokenizer on dataset:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41000/48818 [00:18<00:03, 2232.61 examples/s]Running tokenizer on dataset:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 42000/48818 [00:18<00:03, 2231.62 examples/s]Running tokenizer on dataset:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 43000/48818 [00:18<00:02, 2250.43 examples/s]Running tokenizer on dataset:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 44000/48818 [00:19<00:02, 2246.86 examples/s]Running tokenizer on dataset:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 45000/48818 [00:19<00:01, 2246.54 examples/s]Running tokenizer on dataset:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46000/48818 [00:20<00:01, 2250.65 examples/s]Running tokenizer on dataset:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 47000/48818 [00:20<00:00, 2252.05 examples/s]Running tokenizer on dataset:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 48000/48818 [00:21<00:00, 2254.49 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48818/48818 [00:21<00:00, 2253.33 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48818/48818 [00:21<00:00, 2262.36 examples/s]
[INFO|training_args.py:1997] 2024-04-23 16:57:45,524 >> PyTorch: setting up devices
/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/transformers/training_args.py:1910: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:626] 2024-04-23 16:57:47,344 >> Using auto half precision backend
[INFO|trainer.py:2048] 2024-04-23 16:57:47,508 >> ***** Running training *****
[INFO|trainer.py:2049] 2024-04-23 16:57:47,508 >>   Num examples = 48,818
[INFO|trainer.py:2050] 2024-04-23 16:57:47,508 >>   Num Epochs = 3
[INFO|trainer.py:2051] 2024-04-23 16:57:47,508 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2054] 2024-04-23 16:57:47,508 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2055] 2024-04-23 16:57:47,508 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2056] 2024-04-23 16:57:47,508 >>   Total optimization steps = 9,153
[INFO|trainer.py:2057] 2024-04-23 16:57:47,510 >>   Number of trainable parameters = 3,407,872
input_ids:
[1, 330, 10706, 1444, 264, 13903, 2188, 304, 396, 18278, 10895, 13892, 28723, 415, 13892, 5212, 10865, 28725, 10537, 28725, 304, 27057, 11194, 298, 272, 2188, 28742, 28713, 4224, 28723, 28705, 13, 10649, 28747, 28705, 29321, 29569, 30945, 30720, 28914, 29492, 28995, 29279, 29073, 28944, 13, 7226, 11143, 28747, 28705, 28705, 29074, 29061, 28971, 29321, 29569, 30945, 30720, 28914, 29492, 28995, 29279, 29073, 28994, 13, 13, 28740, 28723, 28705, 29321, 29569, 29894, 29337, 29785, 29129, 28944, 29513, 29354, 29949, 30640, 29162, 28914, 29894, 29337, 29798, 29129, 28924, 29118, 31426, 29783, 29041, 235, 186, 148, 29783, 29355, 30239, 233, 182, 182, 28924, 29084, 231, 194, 134, 29258, 29574, 30855, 29497, 30945, 30720, 28924, 29660, 30283, 235, 133, 143, 31736, 29588, 29195, 28924, 29457, 28998, 30278, 29160, 30608, 29805, 29337, 29249, 28944, 13, 13, 28750, 28723, 28705, 30637, 31632, 236, 168, 177, 30716, 28944, 29513, 29354, 30716, 28963, 29056, 236, 181, 159, 28914, 235, 151, 175, 30038, 29041, 29692, 29098, 29041, 29374, 30894, 29535, 29131, 235, 135, 133, 235, 133, 173, 29872, 29195, 30456, 28914, 235, 158, 142, 29917, 30625, 30716, 29535, 28924, 31059, 30596, 29366, 234, 182, 153, 29041, 29366, 235, 135, 133, 235, 133, 173, 29131, 29058, 29487, 30716, 29346, 28924, 29074, 29321, 29569, 30945, 30720, 28914, 236, 168, 177, 30716, 30978, 233, 134, 178, 28944, 13, 13, 28770, 28723, 28705, 234, 160, 164, 234, 159, 163, 30282, 30303, 28944, 234, 160, 164, 234, 159, 163, 29051, 29086, 29337, 30945, 30720, 30067, 29229, 29249, 29059, 28924, 29025, 29356, 29086, 29513, 29354, 29298, 29321, 29425, 28705, 28787, 28733, 28783, 28705, 29138, 29007, 28914, 234, 160, 164, 234, 159, 163, 28944, 30868, 29530, 28914, 234, 160, 164, 234, 159, 163, 28998, 30278, 29160, 30608, 31757, 30563, 29588, 28924, 231, 194, 134, 29258, 29894, 29337, 30999, 29284, 28924, 29457, 29279, 29366, 29303, 29462, 29588, 29131, 29442, 232, 194, 137, 29588, 28944, 32000]
inputs:
<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. 
 Human: ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºã€‚
Assistant:  ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š

1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³ï¼Œèƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·ï¼Œå¢å¼ºè‚Œè‚‰åŠ›é‡ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚

2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©ï¼Œé¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“ï¼Œä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚

3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦ï¼Œæˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›ï¼Œä¿ƒè¿›èº«ä½“æ¢å¤ï¼Œå¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 28705, 29074, 29061, 28971, 29321, 29569, 30945, 30720, 28914, 29492, 28995, 29279, 29073, 28994, 13, 13, 28740, 28723, 28705, 29321, 29569, 29894, 29337, 29785, 29129, 28944, 29513, 29354, 29949, 30640, 29162, 28914, 29894, 29337, 29798, 29129, 28924, 29118, 31426, 29783, 29041, 235, 186, 148, 29783, 29355, 30239, 233, 182, 182, 28924, 29084, 231, 194, 134, 29258, 29574, 30855, 29497, 30945, 30720, 28924, 29660, 30283, 235, 133, 143, 31736, 29588, 29195, 28924, 29457, 28998, 30278, 29160, 30608, 29805, 29337, 29249, 28944, 13, 13, 28750, 28723, 28705, 30637, 31632, 236, 168, 177, 30716, 28944, 29513, 29354, 30716, 28963, 29056, 236, 181, 159, 28914, 235, 151, 175, 30038, 29041, 29692, 29098, 29041, 29374, 30894, 29535, 29131, 235, 135, 133, 235, 133, 173, 29872, 29195, 30456, 28914, 235, 158, 142, 29917, 30625, 30716, 29535, 28924, 31059, 30596, 29366, 234, 182, 153, 29041, 29366, 235, 135, 133, 235, 133, 173, 29131, 29058, 29487, 30716, 29346, 28924, 29074, 29321, 29569, 30945, 30720, 28914, 236, 168, 177, 30716, 30978, 233, 134, 178, 28944, 13, 13, 28770, 28723, 28705, 234, 160, 164, 234, 159, 163, 30282, 30303, 28944, 234, 160, 164, 234, 159, 163, 29051, 29086, 29337, 30945, 30720, 30067, 29229, 29249, 29059, 28924, 29025, 29356, 29086, 29513, 29354, 29298, 29321, 29425, 28705, 28787, 28733, 28783, 28705, 29138, 29007, 28914, 234, 160, 164, 234, 159, 163, 28944, 30868, 29530, 28914, 234, 160, 164, 234, 159, 163, 28998, 30278, 29160, 30608, 31757, 30563, 29588, 28924, 231, 194, 134, 29258, 29894, 29337, 30999, 29284, 28924, 29457, 29279, 29366, 29303, 29462, 29588, 29131, 29442, 232, 194, 137, 29588, 28944, 32000]
labels:
<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> ä»¥ä¸‹æ˜¯ä¿æŒå¥åº·çš„ä¸‰ä¸ªæç¤ºï¼š

1. ä¿æŒèº«ä½“æ´»åŠ¨ã€‚æ¯å¤©åšé€‚å½“çš„èº«ä½“è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€è·‘æ­¥æˆ–æ¸¸æ³³ï¼Œèƒ½ä¿ƒè¿›å¿ƒè¡€ç®¡å¥åº·ï¼Œå¢å¼ºè‚Œè‚‰åŠ›é‡ï¼Œå¹¶æœ‰åŠ©äºå‡å°‘ä½“é‡ã€‚

2. å‡è¡¡é¥®é£Ÿã€‚æ¯å¤©é£Ÿç”¨æ–°é²œçš„è”¬èœã€æ°´æœã€å…¨è°·ç‰©å’Œè„‚è‚ªå«é‡ä½çš„è›‹ç™½è´¨é£Ÿç‰©ï¼Œé¿å…é«˜ç³–ã€é«˜è„‚è‚ªå’ŒåŠ å·¥é£Ÿå“ï¼Œä»¥ä¿æŒå¥åº·çš„é¥®é£Ÿä¹ æƒ¯ã€‚

3. ç¡çœ å……è¶³ã€‚ç¡çœ å¯¹äººä½“å¥åº·è‡³å…³é‡è¦ï¼Œæˆå¹´äººæ¯å¤©åº”ä¿è¯ 7-8 å°æ—¶çš„ç¡çœ ã€‚è‰¯å¥½çš„ç¡çœ æœ‰åŠ©äºå‡è½»å‹åŠ›ï¼Œä¿ƒè¿›èº«ä½“æ¢å¤ï¼Œå¹¶æé«˜æ³¨æ„åŠ›å’Œè®°å¿†åŠ›ã€‚<|im_end|>
  0%|          | 0/9153 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/share/home/22351111/lyc_LoRA_Test/src/train_bash.py", line 14, in <module>
    main()
  File "/share/home/22351111/lyc_LoRA_Test/src/train_bash.py", line 5, in main
    run_exp()
  File "/share/home/22351111/lyc_LoRA_Test/src/llmtuner/tuner/tune.py", line 26, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/share/home/22351111/lyc_LoRA_Test/src/llmtuner/tuner/sft/workflow.py", line 63, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
  File "/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/transformers/trainer.py", line 2249, in _inner_training_loop
    _grad_norm = self.accelerator.clip_grad_norm_(
  File "/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/accelerate/accelerator.py", line 2157, in clip_grad_norm_
    self.unscale_gradients()
  File "/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/accelerate/accelerator.py", line 2107, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 336, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
  File "/share/home/22351111/anaconda3/envs/llama_peft_demand1/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 258, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
  0%|          | 0/9153 [00:05<?, ?it/s]
